# K8s Benchmark Configuration
# ============================
# Copy this file to config.toml and fill in your credentials:
#   cp config.template.toml config.toml
#
# Then edit config.toml and run: python run.py

[model]
# LLM agent type to use for evaluation
# Options: "GPT-4o", "Qwen/Qwen2.5-72B-Instruct", "ReAct_Agent"
agent_type = "GPT-4o"

# Prompt strategy for the agent
# Options: "base", "cot", "few_shot_basic"
prompt_type = "base"

# Number of GPUs for tensor parallelism (only for local models with vLLM)
num_gpus = 1

[model.azure]
# Azure OpenAI Configuration (for GPT-4o and ReAct_Agent)
endpoint = "https://your-resource.openai.azure.com/"
deployment_name = "your-deployment-name"
api_version = "2024-12-01-preview"
api_key = ""  # Your Azure OpenAI API key (or leave empty to use Azure credential)
scope = "https://cognitiveservices.azure.com/.default"
supports_temperature = false

[model.huggingface]
# Hugging Face Configuration (for local models like Qwen)
# Set HUGGINGFACE_TOKEN environment variable or provide here
token = ""

[benchmark]
# Number of queries to generate for each error type (15 error types total)
num_queries = 10

# Maximum iterations per query
max_iteration = 10

# Generate new benchmark config (true) or use existing (false)
# Set to true for first run, then false to reuse the same benchmark
regenerate = true

# Path for benchmark config file (relative to output_dir)
benchmark_path = "error_config.json"

# Run agent test with multiple models/prompts (true/false)
agent_test = false

[paths]
# Root directory for all output files (logs, results, figures)
# Use relative path for Docker compatibility
output_dir = "results"

# Path to the Google microservices-demo directory
# In Docker with volume mount: /microservices-demo (clone manually)
# Local: /path/to/your/microservices-demo
microservice_dir = "/microservices-demo"

